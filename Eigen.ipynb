{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def setup_script():\n",
    "    \"\"\"\n",
    "    Make sure script runs smoothly\n",
    "    \"\"\"\n",
    "\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "def extract_documents(file_paths):\n",
    "    \"\"\"\n",
    "    Takes a list of file paths as an arg.\n",
    "    Returns a list of documents as strings\n",
    "\n",
    "    When opening the file below we must specifcy\n",
    "    an encoding as if we leave this none it'll use\n",
    "    locale.getpreferredencoding() which might have\n",
    "    unexpected results. In my case for windows\n",
    "    this is 'cp1252'. So it will error as\n",
    "    0x9d isn't defined in cp1252. It's a\n",
    "    'RIGHT DOUBLE QUOTATION MARK'\n",
    "    \"\"\"\n",
    "\n",
    "    documents_dict = {\n",
    "        \"doc_name\": [],\n",
    "        \"raw_doc\": []\n",
    "    }\n",
    "    for file_path in file_paths:\n",
    "        f = open(file_path, \"r\", encoding=\"utf8\")\n",
    "        document = f.read()\n",
    "\n",
    "        doc_name = file_path.split(\"\\\\\")[1].split(\".\")[0]\n",
    "        documents_dict['raw_doc'].append(document)\n",
    "        documents_dict['doc_name'].append(doc_name)\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    return pd.DataFrame(documents_dict)\n",
    "\n",
    "\n",
    "def tokenize_document_words(docs_pdf):\n",
    "    \"\"\"\n",
    "    Splitting each document into words\n",
    "    and adding the result to a column in\n",
    "    the pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    doc_words = [\n",
    "        tokenizer.tokenize(\n",
    "            document\n",
    "        ) for document in docs_pdf.raw_doc\n",
    "    ]\n",
    "    docs_pdf['doc_words'] = doc_words\n",
    "\n",
    "\n",
    "def split_docs_to_sentences(docs_pdf):\n",
    "    \"\"\"\n",
    "    Takes a documents dataframe as an arguement.\n",
    "    Break each document down into sentences\n",
    "    \"\"\"\n",
    "\n",
    "    get_doc_sentences = lambda document: [\n",
    "        sentence.replace(\"\\n\", \"\") for sentence in document.split(\".\")\n",
    "    ]\n",
    "    docs_pdf['sentences'] = docs_pdf.raw_doc.apply(get_doc_sentences)\n",
    "\n",
    "\n",
    "def clean_and_preprocess_docs(docs_pdf):\n",
    "    \"\"\"\n",
    "    Takes a document dataframe.\n",
    "    Preprocesses each of the tokenized columns in the\n",
    "    document pdf.\n",
    "    Pre-processing steps include decaptilizing\n",
    "    and removing stopwords.\n",
    "    \"\"\"\n",
    " \n",
    "    decap_words = lambda document: [\n",
    "        word.lower() for word in document\n",
    "    ]\n",
    "    docs_pdf.doc_words = docs_pdf.doc_words.apply(decap_words)\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    remove_stop_words = lambda document: [\n",
    "        word for word in document if word not in stop_words\n",
    "    ]\n",
    "    docs_pdf.doc_words = docs_pdf.doc_words.apply(remove_stop_words)\n",
    "    \n",
    "\n",
    "def advanced_preprocessing(docs_pdf):\n",
    "    \"\"\"\n",
    "    Stemming and Lemmatization are useful\n",
    "    to really lock into the subject of a\n",
    "    document. If we can reduce words to their\n",
    "    base it means we've a better chance of seeing\n",
    "    patterns then if we treated each varaiton of the\n",
    "    same word as a different word.\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    lemmatize_words = lambda document: [\n",
    "        lemmatizer.lemmatize(word) for word in document\n",
    "    ]\n",
    "    stem_words = lambda document: [\n",
    "        stemmer.stem(word) for word in document\n",
    "    ]\n",
    "    docs_pdf.doc_words = docs_pdf.doc_words.apply(lemmatize_words)\n",
    "    docs_pdf.doc_words = docs_pdf.doc_words.apply(stem_words)\n",
    "\n",
    "\n",
    "def generate_word_summary_data(docs_pdf):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the frequency of eac word for each document\n",
    "    docs_pdf['word_frequencies'] = docs_pdf.doc_words.apply(\n",
    "        nltk.FreqDist\n",
    "    )\n",
    "\n",
    "    get_bigrams = lambda doc_words: [\n",
    "        bigram for bigram in ngrams(doc_words, 2)\n",
    "    ]\n",
    "\n",
    "    get_trigrams = lambda doc_words: [\n",
    "        trigram for trigram in ngrams(doc_words, 3)\n",
    "    ]\n",
    "\n",
    "    # Generate all bigrams and trigrams\n",
    "    docs_pdf['bigrams'] = docs_pdf.doc_words.apply(get_bigrams)\n",
    "    docs_pdf['trigrams'] = docs_pdf.doc_words.apply(get_trigrams)    \n",
    "    \n",
    "    # Get the bigram and trigram frequency\n",
    "    docs_pdf['bigrams_freq'] = docs_pdf.bigrams.apply(Counter)\n",
    "    docs_pdf['trigrams_freq'] = docs_pdf.trigrams.apply(Counter)\n",
    "\n",
    "    \n",
    "def get_summary_for_iterable(word, docs_pdf, iterable_col):\n",
    "    \"\"\"\n",
    "    Takes the word, document dataframe and\n",
    "    the column taking the iterable as an agrement.\n",
    "    Returns an occurance summary for the words occurances in\n",
    "    this iterable column. The iterable columns we\n",
    "    can include here are the sentences, bigrams\n",
    "    and trigrams.\n",
    "    \"\"\"\n",
    "\n",
    "    freq_dict = {\n",
    "        doc:0 for doc in docs_pdf.doc_name\n",
    "    }\n",
    "\n",
    "    # Gather all sentences the word occurs in for each doc\n",
    "    occurances = [\n",
    "        (row.doc_name, obj) for index, row in docs_pdf.iterrows() for obj in row[iterable_col]\n",
    "        if word in obj\n",
    "    ]\n",
    "\n",
    "    # Get the number of sentence occurances for the word in each doc\n",
    "    for obj in occurances:\n",
    "        freq_dict[obj[0]] += 1\n",
    "\n",
    "    # total number of sentences occurances for the word\n",
    "    total_frequency = sum(freq_dict.values())\n",
    "    \n",
    "    return {\n",
    "        \"num_occurances_in_{}_by_doc\".format(iterable_col): freq_dict,\n",
    "        \"total_{}_occurances\".format(iterable_col[:-1]): total_frequency,\n",
    "        iterable_col + \"_occurances\": occurances\n",
    "    }\n",
    "\n",
    "\n",
    "def get_full_word_summary(docs_pdf, word):\n",
    "    \"\"\"\n",
    "    Takes a single word and the doc_pdf\n",
    "    as an arguement and returns a\n",
    "    summary of this word in the\n",
    "    doc corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    word_freq_dict = {\n",
    "        row['doc_name']:row['word_frequencies'][word]\n",
    "        for index, row in docs_pdf.iterrows() if word in row['word_frequencies']\n",
    "    }\n",
    "    total_occurances = sum(word_freq_dict.values())\n",
    "\n",
    "    sentence_summary = get_summary_for_iterable(word, docs_pdf, 'sentences')\n",
    "    bigram_summary = get_summary_for_iterable(word, docs_pdf, 'bigrams')\n",
    "    trigram_summary = get_summary_for_iterable(word, docs_pdf, 'trigrams')\n",
    "\n",
    "    return {\n",
    "        \"word\": word,\n",
    "        \"num_occurances_by_doc\": word_freq_dict,\n",
    "        \"total_occurances\": total_occurances,\n",
    "        **{\n",
    "            key:value\n",
    "            for summary in [sentence_summary, bigram_summary, trigram_summary]\n",
    "            for key, value in summary.items()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def get_tf_idf_matrix(docs_pdf):\n",
    "    \"\"\"\n",
    "    Takes the doc_pdf as an arguement\n",
    "    returns the TF-IDF matrix for\n",
    "    the corpus of documents\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    response = vectorizer.fit_transform(\n",
    "        [\n",
    "            ' '.join(words) for words in docs_pdf.doc_words\n",
    "        ]\n",
    "    )\n",
    "    tf_idf = pd.DataFrame(response.toarray(), columns=vectorizer.get_feature_names())\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "def is_word_of_interest(word, pos_type=\"noun\"):\n",
    "    \"\"\"\n",
    "    Takes word and pos_type as agruement\n",
    "    returns true if the word \n",
    "    \"\"\"\n",
    "    if pos_type == 'noun':\n",
    "        post_list = [\"NN\", \"NNP\", \"NNPS\", \"NST\", \"NNS\"]\n",
    "    elif pos_type in [\"adjective\", \"verb\"]:\n",
    "        post_list = [\n",
    "            \"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\",\n",
    "            \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"\n",
    "        ]\n",
    "        \n",
    "    return (\n",
    "        nltk.pos_tag([word])[0][1] in post_list\n",
    "    )\n",
    "\n",
    "\n",
    "def get_summaries_for_unique_words(docs_pdf):\n",
    "    \"\"\"\n",
    "    Generates a summary for all\n",
    "    unique words in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    unique_words = set([word for words in docs_pdf.doc_words for word in words])\n",
    "    word_summaries = []\n",
    "    for word in unique_words:\n",
    "        word_summaries.append(\n",
    "            get_full_word_summary(\n",
    "                docs_pdf=docs_pdf,\n",
    "                word=word\n",
    "            )\n",
    "        )\n",
    "    return pd.DataFrame(word_summaries)\n",
    "\n",
    "\n",
    "def get_docs_sorted_tf_idf_lookup_dicts(tf_idf, docs_pdf):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tf_idf_lookup_dict = {}\n",
    "    get_doc_name = lambda name: dict(docs_pdf.doc_name)[name]\n",
    "\n",
    "    for index, row in tf_idf.iterrows():\n",
    "        tf_idf_lookup_dict[get_doc_name(row.name)] = {\n",
    "            k: v for k, v in sorted(dict(row).items(), key=lambda item: item[1], reverse=True)\n",
    "        }\n",
    "    return tf_idf_lookup_dict\n",
    "\n",
    "\n",
    "def generate_top30_interesting_words(doc_name, tf_idf_lookup_dict, pos_type):\n",
    "    \"\"\"\n",
    "    Generate the top30 most interesting\n",
    "    nouns and verbs for each of the docs.\n",
    "    Our interest metric coming from the\n",
    "    TF-IDF matrix values and our parts\n",
    "    of speech from the NLTK libary.\n",
    "    Write the summary of these top 30\n",
    "    words to a csv\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    interesting_words = []\n",
    "    for word in tf_idf_lookup_dict[doc_name]:\n",
    "        if len(interesting_words) == 30:\n",
    "            break\n",
    "        else:\n",
    "            if is_word_of_interest(word, pos_type=pos_type):\n",
    "                interesting_words.append(word)\n",
    "                \n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"tf_idf_value\": tf_idf_lookup_dict[doc_name][word],\n",
    "             **get_full_word_summary(docs_pdf, word=word)\n",
    "            } for word in interesting_words\n",
    "        ]\n",
    "    ).to_csv(\"files/outputs/document_specific/{}_top30_interesting_{}s.csv\".format(doc_name, pos_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "setup_script()\n",
    "file_paths = glob.glob(\"./files/inputs/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inital Cleaning and Preprocessing of documents\n",
    "docs_pdf = extract_documents(file_paths)\n",
    "\n",
    "split_docs_to_sentences(docs_pdf=docs_pdf)\n",
    "tokenize_document_words(docs_pdf=docs_pdf)\n",
    "clean_and_preprocess_docs(docs_pdf=docs_pdf)\n",
    "\n",
    "docs_pdf.to_csv(\"files/outputs/overall_document_summary.csv\")\n",
    "\n",
    "generate_word_summary_data(docs_pdf=docs_pdf)\n",
    "word_summary_pdf = get_summaries_for_unique_words(docs_pdf=docs_pdf)\n",
    "\n",
    "word_summary_pdf.to_csv(\"files/outputs/all_words_summary.csv\")\n",
    "\n",
    "tf_idf = get_tf_idf_matrix(docs_pdf=docs_pdf)\n",
    "\n",
    "word_summary_pdf.to_csv(\"files/outputs/tf_idf_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_lookup_dict = get_docs_sorted_tf_idf_lookup_dicts(\n",
    "    tf_idf,\n",
    "    docs_pdf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name in tf_idf_lookup_dict.keys():\n",
    "    generate_top30_interesting_words(\n",
    "        doc_name,\n",
    "        tf_idf_lookup_dict,\n",
    "        pos_type=\"noun\"\n",
    "    )\n",
    "for doc_name in tf_idf_lookup_dict.keys():\n",
    "    generate_top30_interesting_words(\n",
    "        doc_name,\n",
    "        tf_idf_lookup_dict,\n",
    "        pos_type=\"adjective\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
